{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09edee9-afa2-46ac-8925-c787f7349249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas numpy matplotlib scipy statsmodels huggingface-hub transformer-lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb02e2a-58ea-4362-8f79-2f4c423988b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'negation_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 792\u001b[39m\n\u001b[32m    789\u001b[39m     experiments.run_simplified_pipeline()\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 789\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m    788\u001b[39m     experiments = SimplifiedIronicReboundExperiments(device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.bfloat16)\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[43mexperiments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_simplified_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 673\u001b[39m, in \u001b[36mSimplifiedIronicReboundExperiments.run_simplified_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_simplified_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_existing_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# Set memory optimization environment variable\u001b[39;00m\n\u001b[32m    676\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mexpandable_segments:True\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mSimplifiedIronicReboundExperiments.load_existing_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_existing_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnegation_dataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'negation_dataset.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import random\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch.nn.functional as F\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN not set. Gated models may fail to load.\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class SimplifiedIronicReboundExperiments:\n",
    "    \n",
    "    def __init__(self, device='cuda', dtype=torch.bfloat16):\n",
    "        self.device = device\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"Warning: CUDA not available, falling back to CPU.\")\n",
    "            self.device = 'cpu'\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def load_existing_dataset(self) -> pd.DataFrame:\n",
    "        df = pd.read_csv('negation_dataset.csv')\n",
    "        return df\n",
    "    \n",
    "    def _get_model_internal_proxies(self, model: HookedTransformer, forbidden_concepts: List[str]) -> Dict[str, str]:\n",
    "        proxies = {}\n",
    "        embed_matrix = model.embed.W_E.to(self.device)\n",
    "        \n",
    "        for forbidden in set(forbidden_concepts):\n",
    "            try:\n",
    "                # Attempt to get the token ID for the forbidden concept. If it's multi-token, use the first token.\n",
    "                # This addresses the 'Input string is not a single token' error.\n",
    "                try:\n",
    "                    forbidden_token = model.to_single_token(forbidden)\n",
    "                except Exception:\n",
    "                    # Fallback for multi-token concepts: use the first token's ID\n",
    "                    forbidden_token = model.to_tokens(forbidden, prepend_bos=False).flatten()[0].item()\n",
    "                forbidden_emb = embed_matrix[forbidden_token]\n",
    "                similarities = F.cosine_similarity(forbidden_emb.unsqueeze(0), embed_matrix, dim=1)\n",
    "                valid_indices = torch.where((similarities > 0.7) & \n",
    "                                          (torch.arange(len(similarities), device=self.device) != forbidden_token))[0]\n",
    "                \n",
    "                if len(valid_indices) > 0:\n",
    "                    proxy_idx = valid_indices[torch.randint(len(valid_indices), (1,), device=self.device)].item()\n",
    "                    proxy_token = model.to_string(proxy_idx)\n",
    "                    proxies[forbidden] = proxy_token\n",
    "                else:\n",
    "                    similarities[forbidden_token] = -1\n",
    "                    best_idx = similarities.argmax().item()\n",
    "                    proxy_token = model.to_string(best_idx)\n",
    "                    proxies[forbidden] = proxy_token\n",
    "            except:\n",
    "                proxies[forbidden] = 'thing'\n",
    "        \n",
    "        return proxies\n",
    "    \n",
    "    def get_optimal_batch_size(self, model_name):\n",
    "        size_mapping = {\n",
    "            #'pythia-410m': 256,\n",
    "            #'bloom-560m': 256,\n",
    "            #'gpt-neox-20b': 8,\n",
    "            'gemma-3-270m': 256,\n",
    "            #'gpt2-small': 256,\n",
    "            #'opt-2.7b': 128,\n",
    "            #'gemma-7b-it': 64,\n",
    "            #'llama-3-8b-instruct': 32,\n",
    "            #'qwen3-14b': 16,\n",
    "            'gpt-oss-20b': 256,\n",
    "            'lfm2-350m': 256\n",
    "        }\n",
    "        return size_mapping.get(model_name, 64)\n",
    "    \n",
    "    def format_prompt(self, text, model_name):\n",
    "        \"\"\"Return prompt string formatted for the target model.\"\"\"\n",
    "        name = model_name.lower()\n",
    "\n",
    "        # Only treat as chat if name explicitly indicates chat/IT\n",
    "        is_chat = (\"-it\" in name) or (\"instruct\" in name)\n",
    "\n",
    "        # Gemma (v2/v3) chat template\n",
    "        if (\"gemma-2\" in name or \"gemma-3\" in name) and is_chat:\n",
    "            return f\"<start_of_turn>user\\n{text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "        # LLaMA 3 chat template\n",
    "        if \"llama-3\" in name and is_chat:\n",
    "            return (\n",
    "                \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "                f\"{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "            )\n",
    "\n",
    "        # Qwen chat template\n",
    "        if \"qwen3\" in name and is_chat:\n",
    "            return f\"<|im_start|>user\\n{text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "        # Default: base models (e.g., gemma-3-270m, lfm2-350m, gpt-oss-20b) → plain text\n",
    "        return text\n",
    "\n",
    "\n",
    "    \n",
    "    def load_single_model(self, model_name: str, model_path: str) -> HookedTransformer:\n",
    "        \"\"\"Loads a single model onto the specified device (GPU only) with OOM handling.\"\"\"\n",
    "        print(f\"Attempting to load model: {model_name} from {model_path}\")\n",
    "        \n",
    "        # Clear CUDA cache first\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model = None\n",
    "        try:\n",
    "            # First attempt: Load to CPU with memory optimization, then move to GPU\n",
    "            model = HookedTransformer.from_pretrained_no_processing(\n",
    "                model_path, \n",
    "                device='cpu',\n",
    "                dtype=self.dtype, \n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True  # Reduce peak RAM usage during loading\n",
    "            )\n",
    "            \n",
    "            # Move to GPU after successful CPU loading\n",
    "            print(f\"Moving {model_name} to GPU...\")\n",
    "            model = model.to(self.device)\n",
    "            \n",
    "            # Validate model on GPU with test input\n",
    "            model.eval()\n",
    "            test_input = torch.tensor([[1, 2, 3]]).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _ = model(test_input)\n",
    "            print(f\"Successfully loaded and validated {model_name} on GPU\")\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"CUDA OOM error for {model_name}: {e}. Trying device_map='auto'...\")\n",
    "            if model is not None:\n",
    "                del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            try:\n",
    "                # Second attempt: Use device_map=\"auto\" for automatic GPU placement\n",
    "                model = HookedTransformer.from_pretrained_no_processing(\n",
    "                    model_path, \n",
    "                    device_map=\"auto\",\n",
    "                    dtype=self.dtype, \n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                \n",
    "                # Validate model with test input\n",
    "                model.eval()\n",
    "                test_input = torch.tensor([[1, 2, 3]]).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    _ = model(test_input)\n",
    "                print(f\"Successfully loaded {model_name} with device_map='auto'\")\n",
    "                return model\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"Second attempt with device_map failed for {model_name}: {e2}\")\n",
    "                if model is not None:\n",
    "                    del model\n",
    "                torch.cuda.empty_cache()\n",
    "                return None\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"Loading interrupted for {model_name}. Cleaning up...\")\n",
    "            if model is not None:\n",
    "                del model\n",
    "            torch.cuda.empty_cache()\n",
    "            raise\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Loading failed for {model_name}: {e}\")\n",
    "            if model is not None:\n",
    "                del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Try redownload as fallback\n",
    "            try:\n",
    "                print(f\"Attempting redownload for {model_name}\")\n",
    "                from huggingface_hub import snapshot_download\n",
    "                import shutil\n",
    "                from pathlib import Path\n",
    "                \n",
    "                cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'\n",
    "                model_cache_path = cache_dir / f\"models--{model_path.replace('/', '--')}\"\n",
    "                if model_cache_path.exists():\n",
    "                    shutil.rmtree(model_cache_path)\n",
    "                    \n",
    "                snapshot_download(repo_id=model_path)\n",
    "                \n",
    "                model = HookedTransformer.from_pretrained_no_processing(\n",
    "                    model_path, \n",
    "                    device='cpu', \n",
    "                    dtype=self.dtype, \n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                return model\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"Redownload also failed for {model_name}: {e2}\")\n",
    "                if model is not None:\n",
    "                    del model\n",
    "                torch.cuda.empty_cache()\n",
    "                return None\n",
    "    \n",
    "    def experiment_e1_mention_controlled_contrast(self, model: HookedTransformer, \n",
    "                                                 dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        forbidden_concepts = dataset['forbidden_concept'].tolist()\n",
    "        proxy_dict = self._get_model_internal_proxies(model, forbidden_concepts)\n",
    "        dataset = dataset.copy()\n",
    "        dataset['proxy_concept'] = dataset['forbidden_concept'].map(proxy_dict)\n",
    "        \n",
    "        results = []\n",
    "        batch_size = self.get_optimal_batch_size(getattr(model.cfg, 'model_name', 'unknown'))\n",
    "        \n",
    "        neg_data = dataset[dataset['prompt_type'] == 'negative'].copy()\n",
    "        neu_data = dataset[dataset['prompt_type'] == 'neutral'].copy()\n",
    "        pos_data = dataset[dataset['prompt_type'] == 'positive'].copy()\n",
    "        \n",
    "        model_name = getattr(model.cfg, 'model_name', 'unknown')\n",
    "        \n",
    "        all_data = []\n",
    "        for df, ptype in [(neg_data, 'negative'), (neu_data, 'neutral'), (pos_data, 'positive')]:\n",
    "            for _, row in df.iterrows():\n",
    "                formatted_prompt = self.format_prompt(row['prompt_text'], model_name)\n",
    "                all_data.append({\n",
    "                    'id': row['id'],\n",
    "                    'prompt_type': ptype,\n",
    "                    'prompt_text': formatted_prompt,\n",
    "                    'forbidden_concept': row['forbidden_concept']\n",
    "                })\n",
    "        \n",
    "        for i in range(0, len(all_data), batch_size):\n",
    "            batch = all_data[i:i+batch_size]\n",
    "            try:\n",
    "                prompts = [item['prompt_text'] for item in batch]\n",
    "                \n",
    "                if hasattr(model, 'tokenizer'):\n",
    "                    tokens = model.tokenizer(prompts, padding=True, return_tensors='pt', \n",
    "                                           truncation=True, max_length=512).to(self.device)\n",
    "                    token_ids = tokens['input_ids']\n",
    "                else:\n",
    "                    token_ids = model.to_tokens(prompts, prepend_bos=True)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(token_ids)[:, -1, :]\n",
    "                    \n",
    "                for j, item in enumerate(batch):\n",
    "                    forbidden = item['forbidden_concept']\n",
    "                    try:\n",
    "                        # Attempt to get the token ID for the forbidden concept. If it's multi-token, use the first token.\n",
    "                        # This addresses the 'Input string is not a single token' error.\n",
    "                        try:\n",
    "                            forbidden_token = model.to_single_token(forbidden)\n",
    "                        except Exception:\n",
    "                            # Fallback for multi-token concepts: use the first token's ID\n",
    "                            forbidden_token = model.to_tokens(forbidden, prepend_bos=False).flatten()[0].item()\n",
    "                        logp = F.log_softmax(logits[j], dim=-1)[forbidden_token].item()\n",
    "                        results.append({\n",
    "                            'id': item['id'],\n",
    "                            'prompt_type': item['prompt_type'],\n",
    "                            'logp': logp,\n",
    "                            'forbidden_concept': forbidden\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not process forbidden concept '{forbidden}' for ID {item['id']}: {e}\")\n",
    "                        results.append({\n",
    "                            'id': item['id'],\n",
    "                            'prompt_type': item['prompt_type'],\n",
    "                            'logp': np.nan,\n",
    "                            'forbidden_concept': forbidden\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Batch {i} failed: {e}. Skipping.\")\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0 or 'logp' not in results_df.columns:\n",
    "            return pd.DataFrame(columns=['id', 'delta', 'logp_neutral', 'logp_negative', 'logp_positive', 'forbidden_concept'])\n",
    "        \n",
    "        # Fill NaN values in logp with the mean of non-NaN values\n",
    "        logp_mean = results_df['logp'].dropna().mean()\n",
    "        if pd.isna(logp_mean):\n",
    "            logp_mean = -10.0  # Default fallback value\n",
    "        results_df['logp'] = results_df['logp'].fillna(logp_mean)\n",
    "        \n",
    "        pivot_df = pd.pivot_table(results_df, values='logp', index='id', \n",
    "                                 columns='prompt_type', fill_value=0, aggfunc='mean')\n",
    "        \n",
    "        delta_results = []\n",
    "        \n",
    "        # Only process negative prompt IDs (0-1665) and match with corresponding neutral/positive\n",
    "        for neg_id in range(len(neg_data)):\n",
    "            neutral_id = neg_id + 1666  # Corresponding neutral prompt ID  \n",
    "            pos_id = neg_id + 3332      # Corresponding positive prompt ID\n",
    "            \n",
    "            # Initialize log probabilities\n",
    "            logp_negative = 0\n",
    "            logp_neutral = 0\n",
    "            logp_positive = np.nan\n",
    "            \n",
    "            # Get negative log probability\n",
    "            if neg_id in pivot_df.index and 'negative' in pivot_df.columns:\n",
    "                logp_negative = pivot_df.loc[neg_id, 'negative']\n",
    "                \n",
    "            # Get neutral log probability  \n",
    "            if neutral_id in pivot_df.index and 'neutral' in pivot_df.columns:\n",
    "                logp_neutral = pivot_df.loc[neutral_id, 'neutral']\n",
    "                \n",
    "            # Get positive log probability with fallback\n",
    "            if pos_id in pivot_df.index and 'positive' in pivot_df.columns:\n",
    "                logp_positive = pivot_df.loc[pos_id, 'positive']\n",
    "                if logp_positive == 0:\n",
    "                    logp_positive = np.nan\n",
    "            else:\n",
    "                if 'positive' in pivot_df.columns:\n",
    "                    pos_values = pivot_df['positive'][pivot_df['positive'] != 0]\n",
    "                    if len(pos_values) > 0:\n",
    "                        logp_positive = pos_values.mean()\n",
    "                    else:\n",
    "                        logp_positive = np.nan\n",
    "                else:\n",
    "                    logp_positive = np.nan\n",
    "            \n",
    "            # Only include if we have both negative and neutral non-zero values\n",
    "            if logp_negative != 0 and logp_neutral != 0:\n",
    "                orig_row = dataset[dataset['id'] == neg_id]\n",
    "                if len(orig_row) > 0:\n",
    "                    forbidden = orig_row.iloc[0]['forbidden_concept']\n",
    "                    delta = logp_neutral - logp_negative\n",
    "                    delta_results.append({\n",
    "                        'id': neg_id,\n",
    "                        'delta': delta,\n",
    "                        'logp_neutral': logp_neutral,\n",
    "                        'logp_negative': logp_negative,\n",
    "                        'logp_positive': logp_positive,\n",
    "                        'forbidden_concept': forbidden\n",
    "                    })\n",
    "        \n",
    "        final_df = pd.DataFrame(delta_results)\n",
    "        \n",
    "        # Ensure no NaN values in final CSV\n",
    "        if len(final_df) > 0:\n",
    "            # Fill NaN values in numeric columns with their means\n",
    "            numeric_cols = ['delta', 'logp_neutral', 'logp_negative', 'logp_positive']\n",
    "            for col in numeric_cols:\n",
    "                if col in final_df.columns:\n",
    "                    col_mean = final_df[col].dropna().mean()\n",
    "                    if pd.isna(col_mean):\n",
    "                        col_mean = -10.0 if 'logp' in col else 0.0\n",
    "                    final_df[col] = final_df[col].fillna(col_mean)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def experiment_e2_load_distractor_effects(self, model: HookedTransformer,\n",
    "                                             dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        results = []\n",
    "        distractor_lengths = [0, 64, 256, 1024, 2048, 4096]\n",
    "        distractor_types = ['syntactic', 'semantic', 'repetition']\n",
    "        \n",
    "        neg_data = dataset[dataset['prompt_type'] == 'negative']\n",
    "        batch_size = self.get_optimal_batch_size(getattr(model.cfg, 'model_name', 'unknown')) // 4\n",
    "        \n",
    "        for dist_type in distractor_types:\n",
    "            for length in distractor_lengths:\n",
    "                for batch_start in range(0, len(neg_data), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(neg_data))\n",
    "                    batch_data = neg_data.iloc[batch_start:batch_end]\n",
    "                    \n",
    "                    try:\n",
    "                        for _, row in batch_data.iterrows():\n",
    "                            if length == 0:\n",
    "                                distractor = \"\"\n",
    "                            elif dist_type == 'syntactic':\n",
    "                                pattern = \" The quick brown fox jumps over the lazy dog.\"\n",
    "                                distractor = pattern * (length // len(pattern.split()))\n",
    "                            elif dist_type == 'semantic':\n",
    "                                related = f\" This {row['topic']} contains many elements and features.\"\n",
    "                                distractor = related * (length // len(related.split()))\n",
    "                            else:\n",
    "                                distractor = f\" {row['prompt_text']}\" * (length // len(row['prompt_text'].split()))\n",
    "                            \n",
    "                            modified_prompt = row['prompt_text'] + distractor\n",
    "                            \n",
    "                            try:\n",
    "                                tokens = model.to_tokens([modified_prompt])\n",
    "                                if tokens.shape[1] > 4096:\n",
    "                                    tokens = tokens[:, :4096]\n",
    "                                    \n",
    "                                # Attempt to get the token ID for the forbidden concept. If it's multi-token, use the first token.\n",
    "                                # This addresses the 'Input string is not a single token' error.\n",
    "                                try:\n",
    "                                    forbidden_token = model.to_single_token(row['forbidden_concept'])\n",
    "                                except Exception:\n",
    "                                    # Fallback for multi-token concepts: use the first token's ID\n",
    "                                    forbidden_token = model.to_tokens(row['forbidden_concept'], prepend_bos=False).flatten()[0].item()\n",
    "                                \n",
    "                                with torch.no_grad():\n",
    "                                    logits = model(tokens)[:, -1, :]\n",
    "                                    logp = F.log_softmax(logits, dim=-1)[0, forbidden_token].item()\n",
    "                                \n",
    "                                results.append({\n",
    "                                    'id': row['id'],\n",
    "                                    'distractor_type': dist_type,\n",
    "                                    'distractor_length': length,\n",
    "                                    'logp': logp,\n",
    "                                    'forbidden_concept': row['forbidden_concept']\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not process forbidden concept '{row['forbidden_concept']}' for ID {row['id']}: {e}\")\n",
    "                                results.append({\n",
    "                                    'id': row['id'],\n",
    "                                    'distractor_type': dist_type,\n",
    "                                    'distractor_length': length,\n",
    "                                    'logp': np.nan,\n",
    "                                    'forbidden_concept': row['forbidden_concept']\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Batch starting at {batch_start} for distractor type {dist_type} and length {length} failed: {e}. Skipping.\")\n",
    "                        continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Fill NaN values in logp with the mean of non-NaN values\n",
    "        if len(results_df) > 0 and 'logp' in results_df.columns:\n",
    "            logp_mean = results_df['logp'].dropna().mean()\n",
    "            if pd.isna(logp_mean):\n",
    "                logp_mean = -10.0  # Default fallback value\n",
    "            results_df['logp'] = results_df['logp'].fillna(logp_mean)\n",
    "        \n",
    "        for dist_type in distractor_types:\n",
    "            type_data = results_df[results_df['distractor_type'] == dist_type]\n",
    "            if len(type_data) > 5:\n",
    "                try:\n",
    "                    x = type_data['distractor_length'].values\n",
    "                    y = type_data['logp'].values\n",
    "                    smoothed = sm.nonparametric.lowess(y, x, frac=0.2)\n",
    "                    type_indices = results_df[results_df['distractor_type'] == dist_type].index\n",
    "                    results_df.loc[type_indices, 'logp_smoothed'] = smoothed[:, 1]\n",
    "                except:\n",
    "                    results_df.loc[results_df['distractor_type'] == dist_type, 'logp_smoothed'] = type_data['logp']\n",
    "        \n",
    "        # Ensure logp_smoothed has no NaN values\n",
    "        if 'logp_smoothed' in results_df.columns:\n",
    "            logp_smoothed_mean = results_df['logp_smoothed'].dropna().mean()\n",
    "            if pd.isna(logp_smoothed_mean):\n",
    "                logp_smoothed_mean = results_df['logp'].mean()\n",
    "            results_df['logp_smoothed'] = results_df['logp_smoothed'].fillna(logp_smoothed_mean)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def compute_statistics_fixed(self, data: pd.DataFrame, value_col: str = 'delta') -> Dict:\n",
    "        values = data[value_col].dropna()\n",
    "        \n",
    "        if len(values) == 0:\n",
    "            return {'mean': 0, 'ci_lower': 0, 'ci_upper': 0, 'p_value': 1.0, 'p_perm': 1.0, 'bootstrap_samples': []}\n",
    "        \n",
    "        bootstrap_samples = []\n",
    "        for _ in range(5000):\n",
    "            sample = np.random.choice(values, size=len(values), replace=True)\n",
    "            bootstrap_samples.append(np.mean(sample))\n",
    "        \n",
    "        ci_lower = np.percentile(bootstrap_samples, 2.5)\n",
    "        ci_upper = np.percentile(bootstrap_samples, 97.5)\n",
    "        \n",
    "        observed_mean = np.mean(values)\n",
    "        perm_stats = []\n",
    "        \n",
    "        for _ in range(1000):\n",
    "            perm_values = values * np.random.choice([-1, 1], size=len(values))\n",
    "            perm_stats.append(np.mean(perm_values))\n",
    "        \n",
    "        p_perm = np.mean(np.abs(perm_stats) >= np.abs(observed_mean))\n",
    "        t_stat, p_value = stats.ttest_1samp(values, 0)\n",
    "        \n",
    "        return {\n",
    "            'mean': observed_mean,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'p_value': p_value,\n",
    "            'p_perm': p_perm,\n",
    "            'n_samples': len(values),\n",
    "            'bootstrap_samples': bootstrap_samples\n",
    "        }\n",
    "    \n",
    "    def validate_experiment_results(self, df, experiment_name, model_name):\n",
    "        issues = []\n",
    "        \n",
    "        # Conditional validation based on experiment type\n",
    "        if experiment_name == 'e1':\n",
    "            # E1 validation: expected 1666 rows and specific columns\n",
    "            expected_rows = 1666\n",
    "            if len(df) != expected_rows:\n",
    "                issues.append(f\"Expected {expected_rows} rows, got {len(df)}\")\n",
    "            \n",
    "            required_cols = ['id', 'delta', 'logp_neutral', 'logp_negative', 'logp_positive', 'forbidden_concept']\n",
    "            missing_cols = set(required_cols) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues.append(f\"Missing columns: {missing_cols}\")\n",
    "            \n",
    "            # Check for zero and NaN values in logp columns\n",
    "            for col in ['logp_neutral', 'logp_negative']:\n",
    "                if col in df.columns:\n",
    "                    zero_count = (df[col] == 0).sum()\n",
    "                    if zero_count > 0:\n",
    "                        issues.append(f\"{col}: {zero_count} zero values found\")\n",
    "                    \n",
    "                    nan_count = df[col].isna().sum()\n",
    "                    if nan_count > 0:\n",
    "                        issues.append(f\"{col}: {nan_count} NaN values found\")\n",
    "            \n",
    "            # Check logp_positive for high NaN percentage\n",
    "            if 'logp_positive' in df.columns:\n",
    "                pos_nan_count = df['logp_positive'].isna().sum()\n",
    "                pos_nan_pct = pos_nan_count / len(df) * 100 if len(df) > 0 else 0\n",
    "                if pos_nan_pct > 20:\n",
    "                    issues.append(f\"logp_positive: {pos_nan_count} NaN values ({pos_nan_pct:.1f}%) - High missing rate\")\n",
    "                    \n",
    "        elif experiment_name == 'e2':\n",
    "            # E2 validation: different columns, no strict row count\n",
    "            required_cols = ['id', 'distractor_type', 'distractor_length', 'logp', 'forbidden_concept']\n",
    "            missing_cols = set(required_cols) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues.append(f\"Missing columns: {missing_cols}\")\n",
    "            \n",
    "            # Check for zero and NaN values in logp\n",
    "            if 'logp' in df.columns:\n",
    "                zero_count = (df['logp'] == 0).sum()\n",
    "                if zero_count > 0:\n",
    "                    issues.append(f\"logp: {zero_count} zero values found\")\n",
    "                \n",
    "                nan_count = df['logp'].isna().sum()\n",
    "                if nan_count > 0:\n",
    "                    issues.append(f\"logp: {nan_count} NaN values found\")\n",
    "            \n",
    "            # Check logp_smoothed if it exists\n",
    "            if 'logp_smoothed' in df.columns:\n",
    "                smoothed_nan_count = df['logp_smoothed'].isna().sum()\n",
    "                if smoothed_nan_count > 0:\n",
    "                    issues.append(f\"logp_smoothed: {smoothed_nan_count} NaN values found\")\n",
    "                    \n",
    "        else:\n",
    "            issues.append(f\"Unknown experiment type: {experiment_name}\")\n",
    "        \n",
    "        # Report results\n",
    "        if issues:\n",
    "            print(f\"VALIDATION ISSUES for {model_name} {experiment_name}:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  ⚠️  {issue}\")\n",
    "        else:\n",
    "            print(f\"✓ Validation passed for {model_name} {experiment_name}\")\n",
    "        \n",
    "        return len(issues) == 0\n",
    "    \n",
    "    def generate_simple_visualizations(self, results_dir: str, model_name: str = \"Model\"):\n",
    "        e1_files = [f for f in os.listdir(results_dir) if f == 'e1.csv']\n",
    "        if e1_files:\n",
    "            e1_data = pd.read_csv(os.path.join(results_dir, e1_files[0]))\n",
    "            \n",
    "            # Validate data before plotting\n",
    "            if len(e1_data) > 0 and 'delta' in e1_data.columns:\n",
    "                delta_values = e1_data['delta'].dropna()\n",
    "                if len(delta_values) > 0:  # Only plot if we have valid data\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    mean_delta = delta_values.mean()\n",
    "                    \n",
    "                    plt.hist(delta_values, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "                    plt.axvline(x=mean_delta, color='red', linestyle='--', linewidth=2, \n",
    "                               label=f'Average = {mean_delta:.1f}')\n",
    "                    \n",
    "                    plt.xlabel('Delta Score (Difference Between Responses)', fontsize=14)\n",
    "                    plt.ylabel('Number of Examples', fontsize=14)\n",
    "                    \n",
    "                    model_display = model_name.upper() if 'gpt2' in model_name.lower() else model_name.title()\n",
    "                    if 'opt' in model_name.lower():\n",
    "                        model_display = model_name.upper()\n",
    "                    elif 'gemma' in model_name.lower():\n",
    "                        model_display = model_name.title()\n",
    "                        \n",
    "                    plt.title(f'{model_display}: How Much Models Distinguish Between Different Responses', \n",
    "                             fontsize=16, fontweight='bold')\n",
    "                    \n",
    "                    textstr = f'Higher scores = Model can better distinguish\\nbetween positive, negative, and neutral responses\\nTotal examples: {len(delta_values):,}'\n",
    "                    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "                    plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=11,\n",
    "                            verticalalignment='top', bbox=props)\n",
    "                    \n",
    "                    plt.legend(fontsize=12)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(results_dir, 'e1.png'), dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    print(f\"E1 visualization saved for {model_name}\")\n",
    "                else:\n",
    "                    print(f\"No valid delta values found for {model_name} E1 visualization\")\n",
    "            else:\n",
    "                print(f\"Invalid E1 data for {model_name} visualization\")\n",
    "        \n",
    "        e2_files = [f for f in os.listdir(results_dir) if f == 'e2.csv']\n",
    "        if e2_files:\n",
    "            e2_data = pd.read_csv(os.path.join(results_dir, e2_files[0]))\n",
    "            \n",
    "            # Validate data before plotting\n",
    "            if len(e2_data) > 0 and 'distractor_type' in e2_data.columns and 'logp' in e2_data.columns:\n",
    "                valid_data = e2_data.dropna(subset=['logp', 'distractor_type', 'distractor_length'])\n",
    "                if len(valid_data) > 0:\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    \n",
    "                    colors = ['blue', 'red', 'green']\n",
    "                    plot_generated = False\n",
    "                    \n",
    "                    for i, dist_type in enumerate(valid_data['distractor_type'].unique()):\n",
    "                        type_data = valid_data[valid_data['distractor_type'] == dist_type]\n",
    "                        \n",
    "                        length_stats = {}\n",
    "                        for length in sorted(type_data['distractor_length'].unique()):\n",
    "                            length_data = type_data[type_data['distractor_length'] == length]['logp'].values\n",
    "                            if len(length_data) > 1:\n",
    "                                stats = self.compute_statistics_fixed(pd.DataFrame({'delta': length_data}), 'delta')\n",
    "                                length_stats[length] = {\n",
    "                                    'mean': stats['mean'],\n",
    "                                    'ci_lower': np.percentile(stats['bootstrap_samples'], 2.5),\n",
    "                                    'ci_upper': np.percentile(stats['bootstrap_samples'], 97.5)\n",
    "                                }\n",
    "                        \n",
    "                        if length_stats:\n",
    "                            lengths = sorted(length_stats.keys())\n",
    "                            means = [length_stats[l]['mean'] for l in lengths]\n",
    "                            cis_lower = [length_stats[l]['ci_lower'] for l in lengths]\n",
    "                            cis_upper = [length_stats[l]['ci_upper'] for l in lengths]\n",
    "                            \n",
    "                            plt.plot(lengths, means, marker='o', label=dist_type, \n",
    "                                    linewidth=2, color=colors[i % len(colors)])\n",
    "                            plt.fill_between(lengths, cis_lower, cis_upper,\n",
    "                                           alpha=0.2, color=colors[i % len(colors)])\n",
    "                            plot_generated = True\n",
    "                    \n",
    "                    if plot_generated:\n",
    "                        plt.xlabel('Distractor Length (tokens)')\n",
    "                        plt.ylabel('Log Probability')\n",
    "                        plt.title('E2: Load/Distractor Effects with 95% CI')\n",
    "                        plt.legend()\n",
    "                        plt.savefig(os.path.join(results_dir, 'e2.png'), dpi=300, bbox_inches='tight')\n",
    "                        plt.close()\n",
    "                        print(f\"E2 visualization saved for {model_name}\")\n",
    "                    else:\n",
    "                        plt.close()\n",
    "                        print(f\"No valid data for {model_name} E2 visualization\")\n",
    "                else:\n",
    "                    print(f\"No valid E2 data found for {model_name} visualization\")\n",
    "            else:\n",
    "                print(f\"Invalid E2 data for {model_name} visualization\")\n",
    "    \n",
    "    def run_simplified_pipeline(self):\n",
    "        dataset = self.load_existing_dataset()\n",
    "        \n",
    "        # Set memory optimization environment variable\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "        model_configs = [\n",
    "            #(\"qwen3-14b\", \"Qwen/Qwen2.5-14B-Instruct\"),\n",
    "            #(\"pythia-410m\", \"EleutherAI/pythia-410m\"),\n",
    "            #(\"opt-2.7b\", \"facebook/opt-2.7b\"),\n",
    "            #(\"llama-3-8b-instruct\", \"meta-llama/Meta-Llama-3-8B-Instruct\"),\n",
    "            #(\"gpt2-small\", \"gpt2\"),\n",
    "            #(\"gemma-7b-it\", \"google/gemma-7b-it\"),\n",
    "            #(\"bloom-560m\", \"bigscience/bloom-560m\"),\n",
    "            #(\"gpt-neox-20b\", \"EleutherAI/gpt-neox-20b\"),\n",
    "            (\"gemma-3-270m\", \"google/gemma-3-270m\"),\n",
    "            (\"gpt-oss-20b\", \"openai/gpt-oss-20b\"),\n",
    "            (\"lfm2-350m\", \"LiquidAI/LFM2-350M\")\n",
    "        ]\n",
    "\n",
    "        for model_name, model_path in model_configs:\n",
    "            print(f\"\\n--- Running experiments for model: {model_name} ---\")\n",
    "            \n",
    "            # Clear CUDA cache before loading each model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            try:\n",
    "                model = self.load_single_model(model_name, model_path)\n",
    "\n",
    "                if model is None:\n",
    "                    print(f\"ERROR: Failed to load {model_name}. Skipping to next model.\")\n",
    "                    # Clear cache even on failure\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "                results_dir = f'results_{model_name}_simplified'\n",
    "                os.makedirs(results_dir, exist_ok=True)\n",
    "                \n",
    "                e1_results = None\n",
    "                e2_results = None\n",
    "\n",
    "                try:\n",
    "                    # Experiment E1\n",
    "                    print(f\"Running Experiment E1 for {model_name}...\")\n",
    "                    e1_results = self.experiment_e1_mention_controlled_contrast(model, dataset)\n",
    "                    \n",
    "                    # Validate and save E1 results\n",
    "                    if len(e1_results) > 0:\n",
    "                        e1_results.to_csv(os.path.join(results_dir, 'e1.csv'), index=False)\n",
    "                        self.validate_experiment_results(e1_results, 'e1', model_name)\n",
    "                        print(f\"✓ E1 results saved to {os.path.join(results_dir, 'e1.csv')}\")\n",
    "                    else:\n",
    "                        print(f\"WARNING: E1 experiment produced no results for {model_name}\")\n",
    "\n",
    "                    # Experiment E2\n",
    "                    print(f\"Running Experiment E2 for {model_name}...\")\n",
    "                    e2_results = self.experiment_e2_load_distractor_effects(model, dataset)\n",
    "                    \n",
    "                    # Validate and save E2 results\n",
    "                    if len(e2_results) > 0:\n",
    "                        e2_results.to_csv(os.path.join(results_dir, 'e2.csv'), index=False)\n",
    "                        self.validate_experiment_results(e2_results, 'e2', model_name)\n",
    "                        print(f\"✓ E2 results saved to {os.path.join(results_dir, 'e2.csv')}\")\n",
    "                    else:\n",
    "                        print(f\"WARNING: E2 experiment produced no results for {model_name}\")\n",
    "\n",
    "                    # Generate Visualizations\n",
    "                    print(f\"Generating visualizations for {model_name}...\")\n",
    "                    self.generate_simple_visualizations(results_dir, model_name)\n",
    "\n",
    "                    # Compute and save summary statistics\n",
    "                    if e1_results is not None and len(e1_results) > 0:\n",
    "                        e1_stats = self.compute_statistics_fixed(e1_results, 'delta')\n",
    "                        summary_stats = {\n",
    "                            'model': model_name,\n",
    "                            'e1_mean_delta': e1_stats['mean'],\n",
    "                            'e1_p_perm': e1_stats['p_perm'],\n",
    "                            'e1_ci_lower': e1_stats['ci_lower'],\n",
    "                            'e1_ci_upper': e1_stats['ci_upper'],\n",
    "                            'total_samples_processed': len(dataset),\n",
    "                            'e1_valid_deltas': len(e1_results),\n",
    "                            'e2_samples': len(e2_results) if e2_results is not None else 0\n",
    "                        }\n",
    "                        summary_df = pd.DataFrame([summary_stats])\n",
    "                        summary_df.to_csv(os.path.join(results_dir, 'summary_stats.csv'), index=False)\n",
    "                        print(f\"✓ Summary statistics saved to {os.path.join(results_dir, 'summary_stats.csv')}\")\n",
    "                    \n",
    "                    print(f\"✓ Successfully completed all experiments for {model_name}\")\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"Experiments interrupted for {model_name}. Cleaning up...\")\n",
    "                    raise\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR during experiments for {model_name}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    \n",
    "                finally:\n",
    "                    # Always clean up model from GPU memory\n",
    "                    if model is not None:\n",
    "                        print(f\"Unloading {model_name} and clearing CUDA cache...\")\n",
    "                        del model\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"Model loading interrupted for {model_name}. Moving to cleanup...\")\n",
    "                raise\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR loading {model_name}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Finished processing {model_name}.\")\n",
    "\n",
    "def main():\n",
    "    experiments = SimplifiedIronicReboundExperiments(device='cuda', dtype=torch.bfloat16)\n",
    "    experiments.run_simplified_pipeline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
